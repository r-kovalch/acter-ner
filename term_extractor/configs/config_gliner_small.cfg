[nlp]
lang = "en"
pipeline = ["transformer","gliner"]
batch_size = 128

[paths]
train = "train_full.spacy"
dev   = "val_full.spacy"
vectors = null
init_tok2vec = null

############################
# Transformer encoder
############################
[components.transformer]
factory = "transformer"

[components.transformer.model]
@architectures = "spacy-transformers.TransformerModel.v3"
name = "roberta-large"
tokenizer_config = {"use_fast": true}
transformer_config = {"output_hidden_states": false}
dropout = 0.1
pooling = {"pooling_mode": "mean_pooling"}

############################
# GLiNER component
############################
[components.gliner]
factory = "gliner_spacy"
gliner_model = "urchade/gliner_small"
chunk_size   = 256
labels       = ["TERM"]   # ACTER coarse tag(s) without BIO prefixes
style        = "ent"

############################
# Training arguments
############################
[training]
accumulate_gradient = 2
dropout = 0.1
optimizer = {"@optimizers":"AdamW.v1","beta1":0.9,"beta2":0.999,"weight_decay":0.01,"lr":5e-5}
max_epochs = 3
patience = 2000
eval_frequency = 2000

[initialize]
vectors = null