{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-kovalch/acter-ner/blob/main/notebooks/acter-rerank-min-roberta-large-en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L0GshdMraUH3",
        "outputId": "49a22c61-2340-4971-b890-0507b31257db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5X8eBFzlToi",
        "outputId": "c2cd72cf-229d-47ff-f7c3-62e478cf87e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'acter-ner'...\n",
            "remote: Enumerating objects: 522, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 522 (delta 11), reused 3 (delta 3), pack-reused 491 (from 1)\u001b[K\n",
            "Receiving objects: 100% (522/522), 4.19 MiB | 8.92 MiB/s, done.\n",
            "Resolving deltas: 100% (337/337), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Reennon/acter-ner"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AylaRT/ACTER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anL-YTxVlmw4",
        "outputId": "ef79c6bf-6b2d-4aad-d1be-b6dabaf9c6aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ACTER'...\n",
            "remote: Enumerating objects: 5448, done.\u001b[K\n",
            "remote: Counting objects: 100% (5448/5448), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3304/3304), done.\u001b[K\n",
            "remote: Total 5448 (delta 2684), reused 4893 (delta 2132), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (5448/5448), 11.90 MiB | 14.64 MiB/s, done.\n",
            "Resolving deltas: 100% (2684/2684), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ACTER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfCPdbitmcnO",
        "outputId": "97fc85ce-9a9c-4af2-d70d-bfa7ed327eff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ACTER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/acter-ner/term_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0nNt6v-mJi-",
        "outputId": "7d2bbeec-c49c-4671-dd60-566358339923"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/acter-ner/term_extractor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r train_full.tsv test_full.tsv input_data output"
      ],
      "metadata": {
        "id": "P7fnWynlhCyq",
        "outputId": "7bbc3698-89fe-462d-e7cd-cd22c966d280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'train_full.tsv': No such file or directory\n",
            "rm: cannot remove 'test_full.tsv': No such file or directory\n",
            "rm: cannot remove 'input_data': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/ucu/ner/threshold_datasets/ReRank_min_0.1_train_full.tsv train_full.tsv && \\\n",
        "  cp /content/drive/MyDrive/ucu/ner/threshold_datasets/val_full.tsv test_full.tsv"
      ],
      "metadata": {
        "id": "i0uyL1J6d7i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir output && \\\n",
        "#   spacy convert --converter iob train_full.tsv output && \\\n",
        "#   spacy convert --converter iob test_full.tsv output\n"
      ],
      "metadata": {
        "id": "EPiRUZ_9aztr",
        "outputId": "dc26d18b-7a72-4bfa-9c87-82cb2dfad4d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
            "into documents with `-n 10`.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/spacy\", line 10, in <module>\n",
            "    sys.exit(setup_cli())\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/cli/_util.py\", line 87, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1442, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 740, in main\n",
            "    return _main(\n",
            "           ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 195, in _main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1830, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1226, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 794, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/main.py\", line 698, in wrapper\n",
            "    return callback(**use_params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/cli/convert.py\", line 81, in convert_cli\n",
            "    convert(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/cli/convert.py\", line 146, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/tokens/_serialize.py\", line 85, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/training/converters/conll_ner_to_docs.py\", line 105, in conll_ner_to_docs\n",
            "    raise ValueError(Errors.E903)\n",
            "ValueError: [E903] The token-per-line NER file is not formatted correctly. Try checking whitespace and delimiters. See https://spacy.io/api/cli#convert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell X – programmatic IOB‐TSV → DocBin conversion for multiple splits\n",
        "import spacy\n",
        "from spacy.tokens import DocBin, Span\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Initialize blank pipeline (no models loaded)\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# 2) List of (input TSV, desired output .spacy) pairs\n",
        "splits = {\n",
        "    \"train_full.tsv\":   \"train_full.spacy\",\n",
        "    \"test_full.tsv\": \"test_full.spacy\",\n",
        "}\n",
        "\n",
        "# 3) Prepare output directory\n",
        "out_dir = Path(\"output\")\n",
        "out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# 4) Conversion loop\n",
        "for tsv_name, spacy_name in splits.items():\n",
        "    tsv_path = Path(tsv_name)\n",
        "    if not tsv_path.exists():\n",
        "        print(f\"⚠️  Skipping missing {tsv_name}\")\n",
        "        continue\n",
        "\n",
        "    docbin = DocBin(store_user_data=True)\n",
        "    doc_count = ent_count = 0\n",
        "\n",
        "    # read token-per-line, blank lines separate sentences → we group N sentences into one Doc\n",
        "    # Here we’ll group every sentence as its own Doc (n_sents=1)\n",
        "    words, labels = [], []\n",
        "    with tsv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if words:\n",
        "                    doc = spacy.tokens.Doc(nlp.vocab, words=words)\n",
        "                    ents = []\n",
        "                    # scan IOB labels to recover Span(start, end)\n",
        "                    start = None\n",
        "                    for i, tag in enumerate(labels):\n",
        "                        if tag.startswith(\"B\"):\n",
        "                            if start is not None:\n",
        "                                ents.append(Span(doc, start, i, label=\"TERM\"))\n",
        "                            start = i\n",
        "                        elif tag.startswith(\"I\"):\n",
        "                            # continuation\n",
        "                            continue\n",
        "                        else:  # \"O\" or other\n",
        "                            if start is not None:\n",
        "                                ents.append(Span(doc, start, i, label=\"TERM\"))\n",
        "                                start = None\n",
        "                    # catch final\n",
        "                    if start is not None:\n",
        "                        ents.append(Span(doc, start, len(labels), label=\"TERM\"))\n",
        "\n",
        "                    doc.ents = ents\n",
        "                    docbin.add(doc)\n",
        "                    doc_count += 1\n",
        "                    ent_count += len(ents)\n",
        "                    words, labels = [], []\n",
        "                continue\n",
        "\n",
        "            # parse token and IOB label (label may be \"O\" or \"B-TERM\"/\"I-TERM\")\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            tok, tag = parts\n",
        "            words.append(tok)\n",
        "            # normalize to plain \"B\"/\"I\"/\"O\"\n",
        "            if tag.startswith(\"B\"):\n",
        "                labels.append(\"B\")\n",
        "            elif tag.startswith(\"I\"):\n",
        "                labels.append(\"I\")\n",
        "            else:\n",
        "                labels.append(\"O\")\n",
        "\n",
        "        # flush last sentence if missing trailing blank line\n",
        "        if words:\n",
        "            doc = spacy.tokens.Doc(nlp.vocab, words=words)\n",
        "            ents = []\n",
        "            start = None\n",
        "            for i, tag in enumerate(labels):\n",
        "                if tag == \"B\":\n",
        "                    if start is not None:\n",
        "                        ents.append(Span(doc, start, i, label=\"TERM\"))\n",
        "                    start = i\n",
        "                elif tag == \"O\" and start is not None:\n",
        "                    ents.append(Span(doc, start, i, label=\"TERM\"))\n",
        "                    start = None\n",
        "            if start is not None:\n",
        "                ents.append(Span(doc, start, len(labels), label=\"TERM\"))\n",
        "            doc.ents = ents\n",
        "            docbin.add(doc)\n",
        "            doc_count += 1\n",
        "            ent_count += len(ents)\n",
        "\n",
        "    # write out the DocBin\n",
        "    out_path = out_dir / spacy_name\n",
        "    docbin.to_disk(out_path)\n",
        "    print(f\"✅ Converted {tsv_name} → {out_path} \"\n",
        "          f\"({doc_count} docs, {ent_count} entities)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K52AmYesowiq",
        "outputId": "01a2f1cd-6037-4c53-b5dd-6f2cebea96bb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Converted train_full.tsv → output/train_full.spacy (9831 docs, 32792 entities)\n",
            "✅ Converted test_full.tsv → output/test_full.spacy (11547 docs, 26651 entities)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir input_data && \\\n",
        "  mv output/train_full.spacy input_data/train_full.spacy && \\\n",
        "  mv output/test_full.spacy input_data/test_full.spacy"
      ],
      "metadata": {
        "id": "MW4g58A-pgXk"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'spacy[transformers]' -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hhr_t0BEp5Qs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spacy train configs/config_base.cfg  --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaYbLOFUoqhx",
        "outputId": "294a160b-2aca-46a8-be07-cb5333f40f5c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ No output directory provided\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "2025-05-23 00:26:05.276663: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-23 00:26:05.294484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747959965.315784   10009 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747959965.322337   10009 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-23 00:26:05.343585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 1e-05\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0          36.05     66.12   22.14   12.68   87.35    0.22\n",
            "  0     200        1186.90   1798.89   28.71   24.64   34.40    0.29\n",
            "  0     400         816.46   1090.45   26.69   23.67   30.59    0.27\n",
            "  0     600         595.53    779.31   26.75   23.68   30.73    0.27\n",
            "  0     800         489.47    703.87   26.26   22.68   31.18    0.26\n",
            "  0    1000         455.80    624.70   25.70   23.50   28.37    0.26\n",
            "  0    1200         424.17    602.46   26.50   22.38   32.46    0.26\n",
            "  0    1400         361.58    521.36   24.93   22.20   28.41    0.25\n",
            "  0    1600         383.97    538.44   28.09   24.96   32.11    0.28\n",
            "  0    1800         394.65    531.34   24.97   22.20   28.52    0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "s7z5jjlKd7i5"
      }
    }
  ]
}