{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-kovalch/acter-ner/blob/main/notebooks/acter-gliner-small-en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5X8eBFzlToi",
        "outputId": "de98c9ec-18a3-4526-9ac0-4197202ce770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'acter-ner'...\n",
            "remote: Enumerating objects: 263, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 263 (delta 11), reused 7 (delta 5), pack-reused 231 (from 1)\u001b[K\n",
            "Receiving objects: 100% (263/263), 2.43 MiB | 3.50 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/r-kovalch/acter-ner"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AylaRT/ACTER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anL-YTxVlmw4",
        "outputId": "2b9ae765-1b50-444d-ca2b-274c9b9afa77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ACTER'...\n",
            "remote: Enumerating objects: 5448, done.\u001b[K\n",
            "remote: Counting objects: 100% (5448/5448), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3304/3304), done.\u001b[K\n",
            "remote: Total 5448 (delta 2684), reused 4893 (delta 2132), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (5448/5448), 11.90 MiB | 10.19 MiB/s, done.\n",
            "Resolving deltas: 100% (2684/2684), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ACTER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfCPdbitmcnO",
        "outputId": "392320a0-e454-435f-d377-cdcc78df5391"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ACTER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/acter-ner/term_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0nNt6v-mJi-",
        "outputId": "384fd291-2084-4267-ad5b-e613f3b0dc03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/acter-ner/term_extractor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKm-vAEvmQW9",
        "outputId": "25d1f052-6b61-411a-c7cb-e641bd8119b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combine_corpora-en-fr-nl.sh  dataset_processor.py        train_full.tsv\n",
            "combine_corpora-en-fr.sh     \u001b[0m\u001b[01;34moutput\u001b[0m/                     train_model.py\n",
            "combine_corpora_gliner.sh    preprocess_acter_gliner.py  train_spacy_model.py\n",
            "combine_corpora.sh           preprocess_acter.py         Untitled.ipynb\n",
            "\u001b[01;34mconfigs\u001b[0m/                     test_full.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash combine_corpora_gliner.sh"
      ],
      "metadata": {
        "id": "1WlGm-kRl9tu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spacy convert --converter iob train_full.tsv output && \\\n",
        "  spacy convert --converter iob test_full.tsv output\n"
      ],
      "metadata": {
        "id": "K52AmYesowiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250a3de3-3fac-4880-8d5f-687058a6460e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
            "into documents with `-n 10`.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (11329 documents): output/train_full.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
            "into documents with `-n 10`.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2833 documents): output/test_full.spacy\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p input_data && \\\n",
        "  mv output/train_full.spacy input_data/train_full.spacy && \\\n",
        "  mv output/test_full.spacy input_data/test_full.spacy"
      ],
      "metadata": {
        "id": "MW4g58A-pgXk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd input_data && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhz5NtDedp6t",
        "outputId": "24c544fb-2e28-4a85-ad9e-55a5036194ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_full.spacy  train_full.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'spacy[transformers]' -q\n",
        "!pip install -U \"gliner-spacy>=0.0.11\"  -q # latest release, Jan 2025\n",
        "!pip install -U \"gliner>=0.2.5\"        -q  # core GLiNER lib\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hhr_t0BEp5Qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18bf493-9810-4512-fa48-9c9e8cce1acd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.2/756.2 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m130.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !spacy debug data configs/config_gliner_small.cfg --verbose"
      ],
      "metadata": {
        "id": "2k_qDuZbjiJa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "from spacy.tokens import DocBin\n",
        "db = DocBin().from_disk(\"./input_data/train_full.spacy\")\n",
        "docs = list(db.get_docs(spacy.blank(\"en\").vocab))\n",
        "print(docs[0].text, docs[0].ents)\n"
      ],
      "metadata": {
        "id": "rKMET0D0jg1H",
        "outputId": "29197e2c-06e0-46e1-9c65-0bac3b20daea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".  ()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spacy train configs/config_gliner_small.cfg --output ./model --gpu-id 0"
      ],
      "metadata": {
        "id": "ZaYbLOFUoqhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ca4a19-47a0-4a20-89d3-ae58b5764ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: model\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: model\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "2025-05-12 13:52:50.847803: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747057970.868771    2209 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747057970.875149    2209 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Fetching 10 files:   0% 0/10 [00:00<?, ?it/s]\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 90.5MB/s]\n",
            "\n",
            "gliner_config.json: 100% 676/676 [00:00<00:00, 4.63MB/s]\n",
            "\n",
            "added_tokens.json: 100% 65.0/65.0 [00:00<00:00, 645kB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 970/970 [00:00<00:00, 9.87MB/s]\n",
            "\n",
            "pytorch_model.bin:   0% 0.00/664M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 15.7MB/s]\n",
            "Fetching 10 files:  10% 1/10 [00:00<00:04,  2.00it/s]\n",
            "\n",
            "models_comparison.png: 100% 156k/156k [00:00<00:00, 39.5MB/s]\n",
            "\n",
            "\n",
            "tokenizer.json:   0% 0.00/8.65M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 1.63k/1.63k [00:00<00:00, 13.7MB/s]\n",
            "\n",
            "\n",
            "\n",
            "README.md: 100% 23.3k/23.3k [00:00<00:00, 88.4MB/s]\n",
            "Fetching 10 files:  20% 2/10 [00:01<00:04,  1.97it/s]\n",
            "pytorch_model.bin:   2% 10.5M/664M [00:00<00:42, 15.2MB/s]\u001b[A\n",
            "pytorch_model.bin:   3% 21.0M/664M [00:01<00:33, 19.1MB/s]\u001b[A\n",
            "\n",
            "tokenizer.json: 100% 8.65M/8.65M [00:01<00:00, 6.66MB/s]\u001b[A\u001b[A\n",
            "tokenizer.json: 100% 8.65M/8.65M [00:01<00:00, 6.61MB/s]\n",
            "\n",
            "pytorch_model.bin:   6% 41.9M/664M [00:02<00:28, 21.9MB/s]\u001b[A\n",
            "pytorch_model.bin:   8% 52.4M/664M [00:02<00:27, 22.4MB/s]\u001b[A\n",
            "pytorch_model.bin:   9% 62.9M/664M [00:03<00:30, 19.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  11% 73.4M/664M [00:03<00:28, 20.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  13% 83.9M/664M [00:04<00:27, 21.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  14% 94.4M/664M [00:04<00:25, 22.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  16% 105M/664M [00:04<00:24, 22.4MB/s] \u001b[A\n",
            "pytorch_model.bin:  17% 115M/664M [00:05<00:24, 22.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  19% 126M/664M [00:05<00:23, 22.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  21% 136M/664M [00:06<00:22, 23.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  22% 147M/664M [00:06<00:22, 23.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  24% 157M/664M [00:07<00:21, 23.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  25% 168M/664M [00:07<00:21, 23.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  27% 178M/664M [00:08<00:20, 23.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  28% 189M/664M [00:08<00:20, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  30% 199M/664M [00:08<00:19, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  32% 210M/664M [00:09<00:19, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  33% 220M/664M [00:09<00:19, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  35% 231M/664M [00:10<00:18, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  36% 241M/664M [00:10<00:18, 23.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  38% 252M/664M [00:11<00:17, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  39% 262M/664M [00:11<00:17, 23.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  41% 273M/664M [00:12<00:16, 23.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  43% 283M/664M [00:12<00:16, 23.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  44% 294M/664M [00:13<00:15, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  46% 304M/664M [00:13<00:15, 23.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  47% 315M/664M [00:13<00:15, 23.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  49% 325M/664M [00:14<00:16, 20.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  51% 336M/664M [00:15<00:15, 21.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  52% 346M/664M [00:15<00:14, 22.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  54% 357M/664M [00:15<00:13, 22.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  55% 367M/664M [00:16<00:13, 22.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  57% 377M/664M [00:16<00:12, 22.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  58% 388M/664M [00:17<00:11, 23.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  60% 398M/664M [00:17<00:11, 23.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  62% 409M/664M [00:18<00:10, 23.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  63% 419M/664M [00:18<00:10, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  65% 430M/664M [00:19<00:10, 23.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  66% 440M/664M [00:19<00:09, 23.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  68% 451M/664M [00:19<00:09, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  69% 461M/664M [00:20<00:08, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  71% 472M/664M [00:20<00:08, 23.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  73% 482M/664M [00:21<00:07, 23.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  74% 493M/664M [00:21<00:07, 23.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  76% 503M/664M [00:22<00:06, 23.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  77% 514M/664M [00:22<00:06, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  79% 524M/664M [00:23<00:06, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  81% 535M/664M [00:23<00:05, 23.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  82% 545M/664M [00:24<00:05, 23.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  84% 556M/664M [00:24<00:05, 20.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  85% 566M/664M [00:25<00:05, 16.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  87% 577M/664M [00:26<00:05, 16.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  88% 587M/664M [00:27<00:05, 14.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  90% 598M/664M [00:27<00:04, 14.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  92% 608M/664M [00:28<00:04, 13.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  93% 619M/664M [00:29<00:03, 14.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  95% 629M/664M [00:30<00:02, 14.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  96% 640M/664M [00:30<00:01, 13.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  98% 650M/664M [00:31<00:01, 13.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  99% 661M/664M [00:32<00:00, 12.6MB/s]\u001b[A\n",
            "pytorch_model.bin: 100% 664M/664M [00:33<00:00, 20.0MB/s]\n",
            "Fetching 10 files: 100% 10/10 [00:33<00:00,  3.37s/it]\n",
            "config.json: 100% 578/578 [00:00<00:00, 5.09MB/s]\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['gliner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------  ------  ------  ------\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "  0       0    0.00    0.00    0.00    0.00\n",
            "  0     200    0.00    0.00    0.00    0.00\n",
            "Epoch 1:   0% 0/200 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -v '^$' train_full.tsv | head -n 20\n"
      ],
      "metadata": {
        "id": "Wc7SHQb_pIor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ],
      "metadata": {
        "id": "QWMFPDrSBFQ-"
      }
    }
  ]
}